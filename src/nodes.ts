import { type State, type Update } from "./graph.js";
import { ChatOpenAI } from "@langchain/openai";
import { z } from "zod";
import { HELP, OTHER, QUESTION, SUPPORT } from "../types/types.js";

const model = "gpt-3.5-turbo";

export const processMessage = async (state: State): Promise<Update> => {
  const llm = new ChatOpenAI({
    model,
    apiKey: process.env.OPENAI_API_KEY,
    // don't need to be creative here.
    temperature: 0,
  });

  const structuredLlm = llm.withStructuredOutput(
    z.object({
      type: z.enum([SUPPORT, OTHER]).describe(`
The type of the message. It can be either ${OTHER} or ${SUPPORT}.
`),
    }),
  );

  const res = await structuredLlm.invoke([
    [
      "system",
      `You are an expert message analyzer AI. You are given a message and you need
to categorize the message into either ${OTHER} or ${SUPPORT} category.
`,
    ],
    ["human", state.message.content],
  ]);

  console.log("AI inferred", res.type);

  return {
    messageChoice: res.type,
  };
};

export const processSupport = async (state: State): Promise<Update> => {
  console.log("message in support category", state.message);

  const llm = new ChatOpenAI({
    model,
    apiKey: process.env.OPENAI_API_KEY,
    temperature: 0,
  });

  const structuredLlm = llm.withStructuredOutput(
    z.object({
      type: z.enum([QUESTION, HELP]).describe(`
The type of the support ticket. It can be either ${QUESTION} or ${HELP}.
`),
    }),
  );

  const res = await structuredLlm.invoke([
    [
      "system",
      `
You are an expert support ticket analyzer AI. You are given a message and you need
to categorize the message into either ${QUESTION} or ${HELP} category.
`,
    ],
    ["human", state.message.content],
  ]);

  console.log("AI inferred the support ticket as:", res.type);

  return {
    supportTicket: {
      ...state.supportTicket,
      type: res.type,
    },
  };
};

export const processOther = async (state: State): Promise<Update> => {
  console.log("message in other category", state.message);

  // for now do nothing here.
  // We can send some static output like "sorry, I can't help here"
  return {};
};

export const processSupportHelp = async (state: State): Promise<Update> => {
  console.log("message in support help category", state.message);

  // do something here, like create a support ticket
  // or open a thread tagging some mod
  //
  return {};
};

export const processSupportQuestion = async (state: State): Promise<Update> => {
  console.log("message in support question category", state.message);
  // generate a generic answer from the llm first, then mention the mods
  // or open a thread

  const llm = new ChatOpenAI({
    model,
    apiKey: process.env.OPENAI_API_KEY,
    temperature: 0,
  });

  const res = await llm.invoke([
    [
      "system",
      `
You are an expert support ticket analyzer AI. You are given a message and you need
to generate a answer to this help message.
`,
    ],
    ["human", state.message.content],
  ]);

  let llmResponse: string;
  if (Array.isArray(res.content)) {
    llmResponse = res.content
      .map((item) => {
        if (typeof item === "string") {
          return item;
        }
        return "";
      })
      .join(" ");
  } else if (typeof res.content === "string") {
    llmResponse = res.content;
  } else {
    llmResponse = "No valid response generated by the LLM.";
  }

  return {
    supportTicket: {
      ...state.supportTicket,
      question: {
        title: state.message.content,
        answer: llmResponse,
        links: [],
        ansFound: false,
      },
    },
  };
};

export const processSupportOther = async (state: State): Promise<Update> => {
  console.log("message in support other category", state.message);
  // do something here, like tagging mods
  return {};
};
